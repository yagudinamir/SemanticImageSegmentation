{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CITYSCAPES_DATASET=/home/rvygon/data\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline\n",
    "import skimage\n",
    "from skimage.io import imread, imshow, imsave\n",
    "from tensorflow.python.keras.models import *\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.optimizers import *\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "import time\n",
    "import functools\n",
    "from eval import *\n",
    "from ShowColors import *\n",
    "from ImportUtil import *\n",
    "import os, glob, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "%env CITYSCAPES_DATASET = /home/rvygon/data\n",
    "from tensorflow.metrics import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "EPOCHS_PER_EVAL = 1\n",
    "BATCH_SIZE = 1\n",
    "TOTAL_SIZE = 800\n",
    "VAL_SIZE = 1\n",
    "SCALE_RATE = 2\n",
    "IMG_SHAPE = (int(1024/SCALE_RATE), int(2048/SCALE_RATE), 3)\n",
    "VERBOSE = 1\n",
    "START_INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DeepLab v3 models based on slim library.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.slim.nets import resnet_v2\n",
    "from tensorflow.contrib import layers as layers_lib\n",
    "from tensorflow.contrib.framework.python.ops import arg_scope\n",
    "from tensorflow.contrib.layers.python.layers import layers\n",
    "from tensorflow.contrib.slim.python.slim.nets import resnet_utils\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug\n",
    "\n",
    "\n",
    "\n",
    "last_ten_acc = []\n",
    "_NUM_CLASSES = 20\n",
    "_HEIGHT = IMG_SHAPE[0]\n",
    "_WIDTH = IMG_SHAPE[1]\n",
    "_DEPTH = IMG_SHAPE[2]\n",
    "_MIN_SCALE = 0.5\n",
    "_MAX_SCALE = 2.0\n",
    "_IGNORE_LABEL = 255\n",
    "\n",
    "_POWER = 0.9\n",
    "_MOMENTUM = 0.9\n",
    "\n",
    "_BATCH_NORM_DECAY = 0.9997\n",
    "\n",
    "_NUM_IMAGES = {\n",
    "    'train': 10582,\n",
    "    'validation': 1449,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "_BATCH_NORM_DECAY = 0.9997\n",
    "_WEIGHT_DECAY = 5e-4\n",
    "\n",
    "\n",
    "def atrous_spatial_pyramid_pooling(inputs, output_stride, batch_norm_decay, is_training, depth=256):\n",
    "#with tf.device('/GPU:2'):\n",
    "  \"\"\"Atrous Spatial Pyramid Pooling.\n",
    "\n",
    "  Args:\n",
    "    inputs: A tensor of size [batch, height, width, channels].\n",
    "    output_stride: The ResNet unit's stride. Determines the rates for atrous convolution.\n",
    "      the rates are (6, 12, 18) when the stride is 16, and doubled when 8.\n",
    "    batch_norm_decay: The moving average decay when estimating layer activation\n",
    "      statistics in batch normalization.\n",
    "    is_training: A boolean denoting whether the input is for training.\n",
    "    depth: The depth of the ResNet unit output.\n",
    "\n",
    "  Returns:\n",
    "    The atrous spatial pyramid pooling output.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(\"aspp\"):\n",
    "    if output_stride not in [8, 16]:\n",
    "      raise ValueError('output_stride must be either 8 or 16.')\n",
    "\n",
    "    atrous_rates = [6, 12, 18]\n",
    "    if output_stride == 8:\n",
    "      atrous_rates = [2*rate for rate in atrous_rates]\n",
    "\n",
    "    with tf.contrib.slim.arg_scope(resnet_v2.resnet_arg_scope(batch_norm_decay=batch_norm_decay)):\n",
    "      with arg_scope([layers.batch_norm], is_training=is_training):\n",
    "        inputs_size = tf.shape(inputs)[1:3]\n",
    "        # (a) one 1x1 convolution and three 3x3 convolutions with rates = (6, 12, 18) when output stride = 16.\n",
    "        # the rates are doubled when output stride = 8.\n",
    "        conv_1x1 = layers_lib.conv2d(inputs, depth, [1, 1], stride=1, scope=\"conv_1x1\")\n",
    "        conv_3x3_1 = resnet_utils.conv2d_same(inputs, depth, 3, stride=1, rate=atrous_rates[0], scope='conv_3x3_1')\n",
    "        conv_3x3_2 = resnet_utils.conv2d_same(inputs, depth, 3, stride=1, rate=atrous_rates[1], scope='conv_3x3_2')\n",
    "        conv_3x3_3 = resnet_utils.conv2d_same(inputs, depth, 3, stride=1, rate=atrous_rates[2], scope='conv_3x3_3')\n",
    "        tf.summary.image('activations1', tf.reshape(conv_1x1,(256,64,128,1)))\n",
    "        # (b) the image-level features\n",
    "        with tf.variable_scope(\"image_level_features\"):\n",
    "          # global average pooling\n",
    "          image_level_features = tf.reduce_mean(inputs, [1, 2], name='global_average_pooling', keepdims=True)\n",
    "          # 1x1 convolution with 256 filters( and batch normalization)\n",
    "          image_level_features = layers_lib.conv2d(image_level_features, depth, [1, 1], stride=1, scope='conv_1x1')\n",
    "          # bilinearly upsample features\n",
    "          image_level_features = tf.image.resize_bilinear(image_level_features, inputs_size, name='upsample')\n",
    "          tf.summary.image('activations', tf.reshape(image_level_features,(256,64,128,1)))\n",
    "\n",
    "        net = tf.concat([conv_1x1, conv_3x3_1, conv_3x3_2, conv_3x3_3, tf.cast(image_level_features,tf.float32)], axis=3, name='concat')\n",
    "        net = layers_lib.conv2d(net, depth, [1, 1], stride=1, scope='conv_1x1_concat')\n",
    "\n",
    "        return net\n",
    "\n",
    "\n",
    "def deeplab_v3_generator(num_classes,\n",
    "                         output_stride,\n",
    "                         base_architecture,\n",
    "                         pre_trained_model,\n",
    "                         batch_norm_decay,\n",
    "                         data_format='channels_last'):\n",
    "  \n",
    "      \"\"\"Generator for DeepLab v3 models.\n",
    "\n",
    "      Args:\n",
    "        num_classes: The number of possible classes for image classification.\n",
    "        output_stride: The ResNet unit's stride. Determines the rates for atrous convolution.\n",
    "          the rates are (6, 12, 18) when the stride is 16, and doubled when 8.\n",
    "        base_architecture: The architecture of base Resnet building block.\n",
    "        pre_trained_model: The path to the directory that contains pre-trained models.\n",
    "        batch_norm_decay: The moving average decay when estimating layer activation\n",
    "          statistics in batch normalization.\n",
    "        data_format: The input format ('channels_last', 'channels_first', or None).\n",
    "          If set to None, the format is dependent on whether a GPU is available.\n",
    "          Only 'channels_last' is supported currently.\n",
    "\n",
    "      Returns:\n",
    "        The model function that takes in `inputs` and `is_training` and\n",
    "        returns the output tensor of the DeepLab v3 model.\n",
    "      \"\"\"\n",
    "      if data_format is None:\n",
    "        # data_format = (\n",
    "        #     'channels_first' if tf.test.is_built_with_cuda() else 'channels_last')\n",
    "        pass\n",
    "\n",
    "      if batch_norm_decay is None:\n",
    "        batch_norm_decay = _BATCH_NORM_DECAY\n",
    "\n",
    "      if base_architecture not in ['resnet_v2_50', 'resnet_v2_101']:\n",
    "        raise ValueError(\"'base_architrecture' must be either 'resnet_v2_50' or 'resnet_v2_101'.\")\n",
    "\n",
    "      if base_architecture == 'resnet_v2_50':\n",
    "        base_model = resnet_v2.resnet_v2_50\n",
    "      else:\n",
    "        base_model = resnet_v2.resnet_v2_101\n",
    "\n",
    "      def model(inputs, is_training):\n",
    "        \"\"\"Constructs the ResNet model given the inputs.\"\"\"\n",
    "        if data_format == 'channels_first':\n",
    "          # Convert the inputs from channels_last (NHWC) to channels_first (NCHW).\n",
    "          # This provides a large performance boost on GPU. See\n",
    "          # https://www.tensorflow.org/performance/performance_guide#data_formats\n",
    "          inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
    "\n",
    "        # tf.logging.info('net shape: {}'.format(inputs.shape))\n",
    "\n",
    "        with tf.contrib.slim.arg_scope(resnet_v2.resnet_arg_scope(batch_norm_decay=batch_norm_decay)):\n",
    "          logits, end_points = base_model(inputs,\n",
    "                                          num_classes=None,\n",
    "                                          is_training=is_training,\n",
    "                                          global_pool=False,\n",
    "                                          output_stride=output_stride)\n",
    "\n",
    "        if is_training:\n",
    "          exclude = [base_architecture + '/logits', 'global_step']\n",
    "          variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=exclude)\n",
    "          tf.train.init_from_checkpoint(pre_trained_model,\n",
    "                                        {v.name.split(':')[0]: v for v in variables_to_restore})\n",
    "\n",
    "        inputs_size = tf.shape(inputs)[1:3]\n",
    "        net = end_points[base_architecture + '/block4']\n",
    "        net = atrous_spatial_pyramid_pooling(net, output_stride, batch_norm_decay, is_training)\n",
    "        with tf.variable_scope(\"upsampling_logits\"):\n",
    "          net = layers_lib.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, scope='conv_1x1')\n",
    "          logits = tf.image.resize_bilinear(net, inputs_size, name='upsample')\n",
    "\n",
    "        return logits\n",
    "\n",
    "      return model\n",
    "\n",
    "def deeplabv3_model_fn(features, labels, mode, params):\n",
    "  #images = tf.cast(features,tf.uint8)\n",
    "  #images = tf.cast(\n",
    "    \n",
    "  #    tf.map_fn(preprocessing.mean_image_addition, features),\n",
    "  #    tf.uint8)\n",
    "  images=tf.cast(features,tf.uint8)\n",
    "  network = deeplab_v3_generator(params['num_classes'],\n",
    "                                 params['output_stride'],\n",
    "                                 params['base_architecture'],\n",
    "                                 params['pre_trained_model'],\n",
    "                                 params['batch_norm_decay'])\n",
    "    \n",
    "    \n",
    "  logits = network(features, mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  pred_classes = tf.expand_dims(tf.argmax(logits, axis=3, output_type=tf.int32), axis=3)\n",
    "\n",
    "  pred_decoded_labels = tf.py_func(preprocessing.decode_labels,\n",
    "                                   [pred_classes, params['batch_size'], params['num_classes']],\n",
    "                                   tf.uint8)\n",
    "\n",
    "  predictions = {\n",
    "      'classes': pred_classes,\n",
    "      'probabilities': tf.nn.softmax(logits, name='softmax_tensor'),\n",
    "      'decoded_labels': pred_decoded_labels,      \n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    # Delete 'decoded_labels' from predictions because custom functions produce error when used with saved_model\n",
    "    predictions_without_decoded_labels = predictions.copy()\n",
    "    del predictions_without_decoded_labels['decoded_labels']\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        export_outputs={\n",
    "            'preds': tf.estimator.export.PredictOutput(\n",
    "                predictions)\n",
    "        })\n",
    "\n",
    "  gt_decoded_labels = tf.py_func(preprocessing.decode_labels,\n",
    "                                 [labels, params['batch_size'], params['num_classes']], tf.uint8)\n",
    "\n",
    "  labels = tf.squeeze(labels, axis=3)  # reduce the channel dimension.\n",
    "\n",
    "  logits_by_num_classes = tf.reshape(logits, [-1, params['num_classes']])\n",
    "  labels_flat = tf.reshape(labels, [-1, ])\n",
    "\n",
    "  valid_indices = tf.to_int32(labels_flat <= params['num_classes'] - 1)\n",
    "  valid_logits = tf.dynamic_partition(logits_by_num_classes, valid_indices, num_partitions=2)[1]\n",
    "  valid_labels = tf.dynamic_partition(labels_flat, valid_indices, num_partitions=2)[1]\n",
    "\n",
    "  preds_flat = tf.reshape(pred_classes, [-1, ])\n",
    "  valid_preds = tf.dynamic_partition(preds_flat, valid_indices, num_partitions=2)[1]\n",
    "  confusion_matrix = tf.confusion_matrix(valid_labels, valid_preds, num_classes=params['num_classes'])\n",
    "\n",
    "  predictions['valid_preds'] = valid_preds\n",
    "  predictions['valid_labels'] = valid_labels\n",
    "  predictions['confusion_matrix'] = confusion_matrix\n",
    "\n",
    "  cross_entropy = tf.losses.sparse_softmax_cross_entropy(\n",
    "      logits=valid_logits, labels=tf.cast(valid_labels,tf.int32))\n",
    "  tf.summary.scalar('trainable_params', np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))\n",
    "  tf.logging.info(np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))\n",
    "  # Create a tensor named cross_entropy for logging purposes.\n",
    "  tf.identity(cross_entropy, name='cross_entropy')\n",
    "  tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "  \n",
    "  if not params['freeze_batch_norm']:\n",
    "    train_var_list = [v for v in tf.trainable_variables()]\n",
    "  else:\n",
    "    train_var_list = [v for v in tf.trainable_variables()\n",
    "                      if 'beta' not in v.name and 'gamma' not in v.name]\n",
    "\n",
    "  # Add weight decay to the loss.\n",
    "  with tf.variable_scope(\"total_loss\"):\n",
    "    loss = cross_entropy + params.get('weight_decay', _WEIGHT_DECAY) * tf.add_n(\n",
    "        [tf.nn.l2_loss(v) for v in train_var_list])\n",
    "  # loss = tf.losses.get_total_loss()  # obtain the regularization losses as well\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    tf.summary.image('images',\n",
    "                     tf.concat(axis=2, values=[images, gt_decoded_labels, pred_decoded_labels]),\n",
    "                     max_outputs=params['tensorboard_images_max_outputs'])  # Concatenate row-wise.\n",
    "\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    if params['learning_rate_policy'] == 'piecewise':\n",
    "      # Scale the learning rate linearly with the batch size. When the batch size\n",
    "      # is 128, the learning rate should be 0.1.\n",
    "      initial_learning_rate = 0.1 * params['batch_size'] / 128\n",
    "      batches_per_epoch = params['num_train'] / params['batch_size']\n",
    "      # Multiply the learning rate by 0.1 at 100, 150, and 200 epochs.\n",
    "      boundaries = [int(batches_per_epoch * epoch) for epoch in [100, 150, 200]]\n",
    "      values = [initial_learning_rate * decay for decay in [1, 0.1, 0.01, 0.001]]\n",
    "      learning_rate = tf.train.piecewise_constant(\n",
    "          tf.cast(global_step, tf.int16), boundaries, values)\n",
    "    elif params['learning_rate_policy'] == 'poly':\n",
    "      learning_rate = tf.train.polynomial_decay(\n",
    "          params['initial_learning_rate'],\n",
    "          tf.cast(global_step, tf.int16) - params['initial_global_step'],\n",
    "          params['max_iter'], params['end_learning_rate'], power=params['power'])\n",
    "    else:\n",
    "      raise ValueError('Learning rate policy must be \"piecewise\" or \"poly\"')\n",
    "\n",
    "    # Create a tensor named learning_rate for logging purposes\n",
    "    tf.identity(learning_rate, name='learning_rate')\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(\n",
    "        learning_rate=learning_rate,\n",
    "        momentum=params['momentum'])\n",
    "\n",
    "    # Batch norm requires update ops to be added as a dependency to the train_op\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "      train_op = optimizer.minimize(loss, global_step, var_list=train_var_list)\n",
    "  else:\n",
    "    train_op = None\n",
    "  auc = tf.metrics.auc(valid_labels, valid_preds)\n",
    "  tf.identity(auc[0], name='train_auc')\n",
    "  tf.summary.scalar('train_auc', auc[0])\n",
    "  accuracy = tf.metrics.accuracy(\n",
    "      valid_labels, valid_preds)\n",
    "  mean_iou = tf.metrics.mean_iou(valid_labels, valid_preds, params['num_classes'])\n",
    "  metrics = {'px_accuracy': accuracy, 'mean_iou': mean_iou, 'auc' : auc}\n",
    "  tf.get_default_graph().as_graph_def() \n",
    "                \n",
    "  # Create a tensor named train_accuracy for logging purposes\n",
    "  tf.identity(accuracy[1], name='train_px_accuracy')\n",
    "  tf.summary.scalar('train_px_accuracy', accuracy[1])\n",
    "\n",
    "  def compute_mean_iou(total_cm, name='mean_iou'):\n",
    "    \"\"\"Compute the mean intersection-over-union via the confusion matrix.\"\"\"\n",
    "    sum_over_row = tf.to_float(tf.reduce_sum(total_cm, 0))\n",
    "    sum_over_col = tf.to_float(tf.reduce_sum(total_cm, 1))\n",
    "    cm_diag = tf.to_float(tf.diag_part(total_cm))\n",
    "    denominator = sum_over_row + sum_over_col - cm_diag\n",
    "\n",
    "    # The mean is only computed over classes that appear in the\n",
    "    # label or prediction tensor. If the denominator is 0, we need to\n",
    "    # ignore the class.\n",
    "    num_valid_entries = tf.reduce_sum(tf.cast(\n",
    "        tf.not_equal(denominator, 0), dtype=tf.float32))\n",
    "\n",
    "    # If the value of the denominator is 0, set it to 1 to avoid\n",
    "    # zero division.\n",
    "    denominator = tf.where(\n",
    "        tf.greater(denominator, 0),\n",
    "        denominator,\n",
    "        tf.ones_like(denominator))\n",
    "    iou = tf.div(cm_diag, denominator)\n",
    "\n",
    "    for i in range(params['num_classes']):\n",
    "      tf.identity(iou[i], name='train_iou_class{}'.format(i))\n",
    "      tf.summary.scalar('train_iou_class{}'.format(i), iou[i])\n",
    "\n",
    "    # If the number of valid entries is 0 (no classes) we return 0.\n",
    "    result = tf.where(\n",
    "        tf.greater(num_valid_entries, 0),\n",
    "        tf.reduce_sum(iou, name=name) /num_valid_entries,\n",
    "        0)\n",
    "    return result\n",
    "\n",
    "  train_mean_iou = compute_mean_iou(mean_iou[1])\n",
    "\n",
    "  tf.identity(train_mean_iou, name='train_mean_iou')\n",
    "  tf.summary.scalar('train_mean_iou', train_mean_iou)\n",
    "\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=predictions,\n",
    "      loss=loss,\n",
    "      train_op=train_op,\n",
    "eval_metric_ops=metrics)\n",
    "\n",
    "def _inf_generator():\n",
    "    index = 0\n",
    "    while True:                       \n",
    "        x_data = x_test_data[index:index+BATCH_SIZE]       \n",
    "        index+=BATCH_SIZE\n",
    "        \n",
    "        upd_print(str(index) + ' Images')\n",
    "        x_data=np.squeeze(x_data).astype('float32')\n",
    "        yield x_data\n",
    "\n",
    "def _generator():\n",
    "    index = 0\n",
    "    while True:       \n",
    "        batch_input = []\n",
    "        batch_output = []        \n",
    "        x_data = x_train_data[index:index+BATCH_SIZE]\n",
    "        y_data = y_train_data[index:index+BATCH_SIZE]       \n",
    "        \n",
    "        index+=BATCH_SIZE\n",
    "        \n",
    "        if (index >= TOTAL_SIZE):\n",
    "            index = 0\n",
    "        \n",
    "        \n",
    "        x_data=np.squeeze(x_data)\n",
    "        y_data=np.squeeze(y_data)\n",
    "        \n",
    "        y_data=np.expand_dims(y_data,axis=3)       \n",
    "        \n",
    "        yield x_data, y_data\n",
    "        \n",
    "def _inf_input_fn(dataset):\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    images = iterator.get_next()\n",
    "    return images\n",
    "\n",
    "def input_fn(is_training, dataset, batch_size, num_epochs=1):\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    images, labels = iterator.get_next()\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'home/rvygon/SiriusCV/neo', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 800, '_save_checkpoints_secs': None, '_session_config': gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f33f9698630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Start training.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:58069140\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from home/rvygon/SiriusCV/neo/model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into home/rvygon/SiriusCV/neo/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:382: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:cross_entropy = 3.357646, learning_rate = 0.007, train_auc = 0.0, train_mean_iou = 0.005754395, train_px_accuracy = 0.023694992\n",
      "INFO:tensorflow:loss = 25.536423, step = 0\n",
      "INFO:tensorflow:cross_entropy = 0.5956378, learning_rate = 0.0069979, train_auc = 0.0, train_mean_iou = 0.08576513, train_px_accuracy = 0.4168272 (41.338 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.38509107, learning_rate = 0.006995801, train_auc = 0.0, train_mean_iou = 0.12133746, train_px_accuracy = 0.57768375 (19.256 sec)\n",
      "INFO:tensorflow:cross_entropy = 1.2326959, learning_rate = 0.0069937008, train_auc = 0.0, train_mean_iou = 0.123443104, train_px_accuracy = 0.5956359 (19.316 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.70539206, learning_rate = 0.006991601, train_auc = 0.0, train_mean_iou = 0.15017846, train_px_accuracy = 0.63763773 (19.392 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.50611895, learning_rate = 0.006989501, train_auc = 0.0, train_mean_iou = 0.16267915, train_px_accuracy = 0.6751283 (19.442 sec)\n",
      "INFO:tensorflow:cross_entropy = 1.142132, learning_rate = 0.006987401, train_auc = 0.0, train_mean_iou = 0.15961294, train_px_accuracy = 0.6738592 (19.531 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.43938595, learning_rate = 0.0069853, train_auc = 0.0, train_mean_iou = 0.16749269, train_px_accuracy = 0.6977494 (19.642 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.73196816, learning_rate = 0.0069832006, train_auc = 0.0, train_mean_iou = 0.18935196, train_px_accuracy = 0.70621 (19.596 sec)\n",
      "INFO:tensorflow:cross_entropy = 1.0881336, learning_rate = 0.0069811, train_auc = 0.0, train_mean_iou = 0.19288097, train_px_accuracy = 0.7046913 (19.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.461226\n",
      "INFO:tensorflow:cross_entropy = 0.6717907, learning_rate = 0.0069789994, train_auc = 0.0, train_mean_iou = 0.20768206, train_px_accuracy = 0.71635556 (19.673 sec)\n",
      "INFO:tensorflow:loss = 22.79513, step = 100 (216.817 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.7782273, learning_rate = 0.006976899, train_auc = 0.0, train_mean_iou = 0.21342048, train_px_accuracy = 0.72161293 (19.667 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.70271415, learning_rate = 0.006974798, train_auc = 0.0, train_mean_iou = 0.22979903, train_px_accuracy = 0.72781384 (19.642 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.5728468, learning_rate = 0.0069726985, train_auc = 0.0, train_mean_iou = 0.23211476, train_px_accuracy = 0.7365336 (19.651 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.46850616, learning_rate = 0.006970597, train_auc = 0.0, train_mean_iou = 0.23578429, train_px_accuracy = 0.7444036 (19.672 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.5304027, learning_rate = 0.0069684964, train_auc = 0.0, train_mean_iou = 0.23675223, train_px_accuracy = 0.7488984 (19.660 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.69086236, learning_rate = 0.0069663962, train_auc = 0.0, train_mean_iou = 0.24075325, train_px_accuracy = 0.75159174 (19.606 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.42886618, learning_rate = 0.006964295, train_auc = 0.0, train_mean_iou = 0.25277442, train_px_accuracy = 0.7577577 (19.622 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.1785159, learning_rate = 0.006962194, train_auc = 0.0, train_mean_iou = 0.25620717, train_px_accuracy = 0.76820326 (19.660 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.6272522, learning_rate = 0.0069600926, train_auc = 0.0, train_mean_iou = 0.2578176, train_px_accuracy = 0.769905 (19.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.50886\n",
      "INFO:tensorflow:cross_entropy = 0.9806336, learning_rate = 0.0069579915, train_auc = 0.0, train_mean_iou = 0.262667, train_px_accuracy = 0.7649268 (19.685 sec)\n",
      "INFO:tensorflow:loss = 23.042902, step = 200 (196.515 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.5877157, learning_rate = 0.0069558905, train_auc = 0.0, train_mean_iou = 0.26505873, train_px_accuracy = 0.76717335 (19.661 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.43771142, learning_rate = 0.00695379, train_auc = 0.0, train_mean_iou = 0.26827395, train_px_accuracy = 0.7718782 (19.629 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.53573805, learning_rate = 0.006951689, train_auc = 0.0, train_mean_iou = 0.27091438, train_px_accuracy = 0.7737691 (19.635 sec)\n",
      "INFO:tensorflow:cross_entropy = 1.3963087, learning_rate = 0.006949587, train_auc = 0.0, train_mean_iou = 0.27201504, train_px_accuracy = 0.7695724 (19.622 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.5676636, learning_rate = 0.0069474857, train_auc = 0.0, train_mean_iou = 0.2731831, train_px_accuracy = 0.770827 (19.610 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.51970327, learning_rate = 0.0069453837, train_auc = 0.0, train_mean_iou = 0.27726412, train_px_accuracy = 0.7732434 (19.647 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.54276276, learning_rate = 0.0069432827, train_auc = 0.0, train_mean_iou = 0.27864403, train_px_accuracy = 0.7749041 (19.617 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.5176484, learning_rate = 0.006941181, train_auc = 0.0, train_mean_iou = 0.28145474, train_px_accuracy = 0.77751267 (19.638 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.38012093, learning_rate = 0.006939079, train_auc = 0.0, train_mean_iou = 0.2925244, train_px_accuracy = 0.781226 (19.617 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.509411\n",
      "INFO:tensorflow:cross_entropy = 0.33288574, learning_rate = 0.006936978, train_auc = 0.0, train_mean_iou = 0.29306602, train_px_accuracy = 0.7851874 (19.628 sec)\n",
      "INFO:tensorflow:loss = 22.334358, step = 300 (196.305 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.39729106, learning_rate = 0.006934875, train_auc = 0.0, train_mean_iou = 0.29454812, train_px_accuracy = 0.788367 (19.621 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.3436705, learning_rate = 0.0069327736, train_auc = 0.0, train_mean_iou = 0.3027433, train_px_accuracy = 0.7919026 (19.608 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.49891707, learning_rate = 0.006930672, train_auc = 0.0, train_mean_iou = 0.30181903, train_px_accuracy = 0.7930879 (19.662 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.57050574, learning_rate = 0.0069285696, train_auc = 0.0, train_mean_iou = 0.30685472, train_px_accuracy = 0.7941374 (19.651 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.5664807, learning_rate = 0.006926467, train_auc = 0.0, train_mean_iou = 0.3095432, train_px_accuracy = 0.7948182 (19.669 sec)\n",
      "INFO:tensorflow:cross_entropy = 1.2020314, learning_rate = 0.006924365, train_auc = 0.0, train_mean_iou = 0.3098846, train_px_accuracy = 0.7904199 (19.608 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.5764949, learning_rate = 0.006922263, train_auc = 0.0, train_mean_iou = 0.31340396, train_px_accuracy = 0.7916024 (19.626 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.33399945, learning_rate = 0.0069201607, train_auc = 0.0, train_mean_iou = 0.31739944, train_px_accuracy = 0.79443747 (19.696 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.49330145, learning_rate = 0.0069180583, train_auc = 0.0, train_mean_iou = 0.3185206, train_px_accuracy = 0.79514396 (19.634 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.509052\n",
      "INFO:tensorflow:cross_entropy = 0.18569319, learning_rate = 0.006915956, train_auc = 0.0, train_mean_iou = 0.32006294, train_px_accuracy = 0.7987347 (19.669 sec)\n",
      "INFO:tensorflow:loss = 22.126726, step = 400 (196.444 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.3641105, learning_rate = 0.0069138533, train_auc = 0.0, train_mean_iou = 0.3211157, train_px_accuracy = 0.8013353 (19.656 sec)\n",
      "INFO:tensorflow:cross_entropy = 1.4851693, learning_rate = 0.0069117504, train_auc = 0.0, train_mean_iou = 0.31924635, train_px_accuracy = 0.79649067 (19.617 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.3415196, learning_rate = 0.0069096484, train_auc = 0.0, train_mean_iou = 0.32530352, train_px_accuracy = 0.79887044 (19.639 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.26954755, learning_rate = 0.006907545, train_auc = 0.0, train_mean_iou = 0.32667398, train_px_accuracy = 0.8015384 (19.617 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.393893, learning_rate = 0.006905442, train_auc = 0.0, train_mean_iou = 0.3321029, train_px_accuracy = 0.8032303 (19.616 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.5055932, learning_rate = 0.006903339, train_auc = 0.0, train_mean_iou = 0.33273283, train_px_accuracy = 0.8041582 (19.629 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.39005533, learning_rate = 0.0069012367, train_auc = 0.0, train_mean_iou = 0.33298254, train_px_accuracy = 0.80548674 (19.607 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.37406975, learning_rate = 0.006899134, train_auc = 0.0, train_mean_iou = 0.3359541, train_px_accuracy = 0.8070331 (19.638 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:cross_entropy = 0.2694478, learning_rate = 0.0068970304, train_auc = 0.0, train_mean_iou = 0.34296876, train_px_accuracy = 0.8093455 (19.608 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.509501\n",
      "INFO:tensorflow:cross_entropy = 2.3864405, learning_rate = 0.0068949275, train_auc = 0.0, train_mean_iou = 0.34212288, train_px_accuracy = 0.7998416 (19.644 sec)\n",
      "INFO:tensorflow:loss = 24.267344, step = 500 (196.270 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.28014573, learning_rate = 0.006892824, train_auc = 0.0, train_mean_iou = 0.34428006, train_px_accuracy = 0.8024442 (19.682 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.4416137, learning_rate = 0.0068907207, train_auc = 0.0, train_mean_iou = 0.3468086, train_px_accuracy = 0.80312216 (19.641 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.20414984, learning_rate = 0.006888617, train_auc = 0.0, train_mean_iou = 0.3477865, train_px_accuracy = 0.8058154 (19.612 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.20839152, learning_rate = 0.006886514, train_auc = 0.0, train_mean_iou = 0.35005304, train_px_accuracy = 0.8081809 (19.628 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.4163013, learning_rate = 0.00688441, train_auc = 0.0, train_mean_iou = 0.35068652, train_px_accuracy = 0.8092199 (19.623 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.3139339, learning_rate = 0.006882306, train_auc = 0.0, train_mean_iou = 0.3533309, train_px_accuracy = 0.8109045 (19.631 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.30080593, learning_rate = 0.006880203, train_auc = 0.0, train_mean_iou = 0.35426554, train_px_accuracy = 0.8128458 (19.617 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.23634607, learning_rate = 0.006878099, train_auc = 0.0, train_mean_iou = 0.36242908, train_px_accuracy = 0.8149037 (19.625 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.4713704, learning_rate = 0.0068759946, train_auc = 0.0, train_mean_iou = 0.36203423, train_px_accuracy = 0.8156763 (19.619 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.509311\n",
      "INFO:tensorflow:cross_entropy = 0.43756098, learning_rate = 0.0068738908, train_auc = 0.0, train_mean_iou = 0.36268678, train_px_accuracy = 0.8165127 (19.666 sec)\n",
      "INFO:tensorflow:loss = 22.258678, step = 600 (196.344 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.59135145, learning_rate = 0.006871787, train_auc = 0.0, train_mean_iou = 0.3626662, train_px_accuracy = 0.8164266 (19.641 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.3498047, learning_rate = 0.006869683, train_auc = 0.0, train_mean_iou = 0.36433482, train_px_accuracy = 0.8176503 (19.643 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.46539134, learning_rate = 0.006867579, train_auc = 0.0, train_mean_iou = 0.36328095, train_px_accuracy = 0.81874526 (19.607 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.31541684, learning_rate = 0.006865475, train_auc = 0.0, train_mean_iou = 0.3639074, train_px_accuracy = 0.82013714 (19.647 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.5255659, learning_rate = 0.0068633705, train_auc = 0.0, train_mean_iou = 0.36474532, train_px_accuracy = 0.8202625 (19.615 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.37208655, learning_rate = 0.006861266, train_auc = 0.0, train_mean_iou = 0.36480087, train_px_accuracy = 0.82132787 (19.635 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.38229415, learning_rate = 0.006859162, train_auc = 0.0, train_mean_iou = 0.36460042, train_px_accuracy = 0.82202196 (19.610 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.2993884, learning_rate = 0.006857057, train_auc = 0.0, train_mean_iou = 0.36732224, train_px_accuracy = 0.8231099 (19.626 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.39306027, learning_rate = 0.006854953, train_auc = 0.0, train_mean_iou = 0.36849532, train_px_accuracy = 0.82362103 (19.622 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.50936\n",
      "INFO:tensorflow:cross_entropy = 0.16245542, learning_rate = 0.006852848, train_auc = 0.0, train_mean_iou = 0.36913687, train_px_accuracy = 0.8255373 (19.678 sec)\n",
      "INFO:tensorflow:loss = 21.924124, step = 700 (196.325 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.31734133, learning_rate = 0.006850743, train_auc = 0.0, train_mean_iou = 0.3686828, train_px_accuracy = 0.8264704 (19.641 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.21006113, learning_rate = 0.0068486384, train_auc = 0.0, train_mean_iou = 0.36929375, train_px_accuracy = 0.82800156 (19.636 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.32436293, learning_rate = 0.0068465336, train_auc = 0.0, train_mean_iou = 0.36936486, train_px_accuracy = 0.8289104 (19.686 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.32610035, learning_rate = 0.0068444284, train_auc = 0.0, train_mean_iou = 0.37237024, train_px_accuracy = 0.8297041 (19.637 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.35242173, learning_rate = 0.0068423245, train_auc = 0.0, train_mean_iou = 0.37296468, train_px_accuracy = 0.83054 (19.616 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.26769364, learning_rate = 0.006840219, train_auc = 0.0, train_mean_iou = 0.37496024, train_px_accuracy = 0.83159864 (19.610 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.22682415, learning_rate = 0.0068381135, train_auc = 0.0, train_mean_iou = 0.37553552, train_px_accuracy = 0.8328285 (19.637 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.69964004, learning_rate = 0.0068360083, train_auc = 0.0, train_mean_iou = 0.37695545, train_px_accuracy = 0.83226633 (19.607 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.29476464, learning_rate = 0.006833903, train_auc = 0.0, train_mean_iou = 0.37838832, train_px_accuracy = 0.83319724 (19.622 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 800 into home/rvygon/SiriusCV/neo/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.503209\n",
      "INFO:tensorflow:cross_entropy = 0.28993174, learning_rate = 0.0068317982, train_auc = 0.0, train_mean_iou = 0.3789801, train_px_accuracy = 0.8341539 (22.032 sec)\n",
      "INFO:tensorflow:loss = 21.99245, step = 800 (198.724 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 805 into home/rvygon/SiriusCV/neo/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 22.202078.\n",
      "INFO:tensorflow:Epoch ended.\n",
      "INFO:tensorflow:Start training.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:58069140\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from home/rvygon/SiriusCV/neo/model.ckpt-805\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 805 into home/rvygon/SiriusCV/neo/model.ckpt.\n",
      "INFO:tensorflow:cross_entropy = 0.31609464, learning_rate = 0.006830745, train_auc = 0.0, train_mean_iou = 0.49513224, train_px_accuracy = 0.9001789\n",
      "INFO:tensorflow:loss = 22.015667, step = 805\n",
      "INFO:tensorflow:cross_entropy = 0.17001078, learning_rate = 0.0068286397, train_auc = 0.0, train_mean_iou = 0.5365658, train_px_accuracy = 0.9283428 (41.463 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.4029266, learning_rate = 0.006826535, train_auc = 0.0, train_mean_iou = 0.51271904, train_px_accuracy = 0.9078865 (19.164 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.34423578, learning_rate = 0.006824428, train_auc = 0.0, train_mean_iou = 0.49460405, train_px_accuracy = 0.9043546 (19.228 sec)\n",
      "INFO:tensorflow:cross_entropy = 1.0589066, learning_rate = 0.006822323, train_auc = 0.0, train_mean_iou = 0.3942858, train_px_accuracy = 0.84881556 (19.395 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.28061423, learning_rate = 0.006820217, train_auc = 0.0, train_mean_iou = 0.4034629, train_px_accuracy = 0.86005974 (19.435 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.4711635, learning_rate = 0.0068181115, train_auc = 0.0, train_mean_iou = 0.4038158, train_px_accuracy = 0.86157554 (19.487 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.54429466, learning_rate = 0.006816006, train_auc = 0.0, train_mean_iou = 0.3739602, train_px_accuracy = 0.85416794 (19.506 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.24165022, learning_rate = 0.006813899, train_auc = 0.0, train_mean_iou = 0.38041422, train_px_accuracy = 0.86233944 (19.496 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.40897068, learning_rate = 0.0068117934, train_auc = 0.0, train_mean_iou = 0.38952976, train_px_accuracy = 0.86193293 (19.584 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.462217\n",
      "INFO:tensorflow:cross_entropy = 0.2730294, learning_rate = 0.006809687, train_auc = 0.0, train_mean_iou = 0.39183077, train_px_accuracy = 0.8673321 (19.592 sec)\n",
      "INFO:tensorflow:loss = 21.913906, step = 905 (216.350 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:cross_entropy = 0.15301915, learning_rate = 0.006807581, train_auc = 0.0, train_mean_iou = 0.39858788, train_px_accuracy = 0.8746805 (19.589 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.45760924, learning_rate = 0.006805475, train_auc = 0.0, train_mean_iou = 0.39903855, train_px_accuracy = 0.8722405 (19.581 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.24806327, learning_rate = 0.006803368, train_auc = 0.0, train_mean_iou = 0.40052915, train_px_accuracy = 0.8759847 (19.637 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.294492, learning_rate = 0.006801262, train_auc = 0.0, train_mean_iou = 0.41843134, train_px_accuracy = 0.8785791 (19.634 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.42428398, learning_rate = 0.0067991554, train_auc = 0.0, train_mean_iou = 0.41347894, train_px_accuracy = 0.8765451 (19.640 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.34215364, learning_rate = 0.006797049, train_auc = 0.0, train_mean_iou = 0.41293058, train_px_accuracy = 0.8768591 (19.616 sec)\n",
      "INFO:tensorflow:cross_entropy = 1.0407586, learning_rate = 0.0067949425, train_auc = 0.0, train_mean_iou = 0.403411, train_px_accuracy = 0.86117053 (19.640 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.42434782, learning_rate = 0.006792836, train_auc = 0.0, train_mean_iou = 0.40278792, train_px_accuracy = 0.8608806 (19.607 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.73630184, learning_rate = 0.0067907292, train_auc = 0.0, train_mean_iou = 0.42354125, train_px_accuracy = 0.85606307 (19.637 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.509691\n",
      "INFO:tensorflow:cross_entropy = 0.36485583, learning_rate = 0.006788622, train_auc = 0.0, train_mean_iou = 0.42363042, train_px_accuracy = 0.858375 (19.616 sec)\n",
      "INFO:tensorflow:loss = 21.947311, step = 1005 (196.197 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.2647871, learning_rate = 0.0067865155, train_auc = 0.0, train_mean_iou = 0.4343295, train_px_accuracy = 0.86101186 (19.636 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.12334888, learning_rate = 0.0067844084, train_auc = 0.0, train_mean_iou = 0.43588257, train_px_accuracy = 0.8654504 (19.599 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.36577705, learning_rate = 0.0067823012, train_auc = 0.0, train_mean_iou = 0.432458, train_px_accuracy = 0.8654862 (19.627 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.12409447, learning_rate = 0.006780194, train_auc = 0.0, train_mean_iou = 0.43439308, train_px_accuracy = 0.8694925 (19.629 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.36800227, learning_rate = 0.0067780865, train_auc = 0.0, train_mean_iou = 0.4332219, train_px_accuracy = 0.8695966 (19.606 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.1877804, learning_rate = 0.00677598, train_auc = 0.0, train_mean_iou = 0.43406448, train_px_accuracy = 0.8723198 (19.653 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.29274648, learning_rate = 0.0067738723, train_auc = 0.0, train_mean_iou = 0.4309724, train_px_accuracy = 0.8735772 (19.625 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.6949776, learning_rate = 0.006771765, train_auc = 0.0, train_mean_iou = 0.4299333, train_px_accuracy = 0.87123877 (19.632 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.8252388, learning_rate = 0.006769657, train_auc = 0.0, train_mean_iou = 0.4412927, train_px_accuracy = 0.86640346 (19.594 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.509196\n",
      "INFO:tensorflow:cross_entropy = 0.41148922, learning_rate = 0.00676755, train_auc = 0.0, train_mean_iou = 0.43945423, train_px_accuracy = 0.8663751 (19.786 sec)\n",
      "INFO:tensorflow:loss = 21.935783, step = 1105 (196.388 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.4835127, learning_rate = 0.0067654424, train_auc = 0.0, train_mean_iou = 0.4402915, train_px_accuracy = 0.86565 (19.633 sec)\n",
      "INFO:tensorflow:cross_entropy = 0.18314502, learning_rate = 0.0067633344, train_auc = 0.0, train_mean_iou = 0.44112477, train_px_accuracy = 0.86810184 (19.606 sec)\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "# Set up a RunConfig to only save checkpoints once per training cycle.\n",
    "run_config = tf.estimator.RunConfig().replace(session_config=config, save_checkpoints_steps=TOTAL_SIZE)\n",
    "def upd_print(str):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(str)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "model = tf.estimator.Estimator(\n",
    "    model_fn=deeplabv3_model_fn,\n",
    "    config=run_config,\n",
    "    model_dir='home/rvygon/SiriusCV/neo',\n",
    "    params = {\n",
    "        'output_stride': 8,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'base_architecture': 'resnet_v2_101',\n",
    "        'pre_trained_model': '/home/rvygon/data/DeepLab/resnet_v2_101.ckpt',\n",
    "        'batch_norm_decay': _BATCH_NORM_DECAY,\n",
    "        'num_classes': _NUM_CLASSES,\n",
    "        'tensorboard_images_max_outputs': 6,\n",
    "        'weight_decay': 2e-4,\n",
    "        'learning_rate_policy': 'poly',\n",
    "        'num_train': TOTAL_SIZE,\n",
    "        'initial_learning_rate': 7e-3,\n",
    "        'max_iter': 30000,\n",
    "        'end_learning_rate': 1e-6,\n",
    "        'power': _POWER,\n",
    "        'momentum': _MOMENTUM,\n",
    "        'freeze_batch_norm': False,\n",
    "        'initial_global_step': 0\n",
    "      })\n",
    "\n",
    "for _ in range(150):\n",
    "    tensors_to_log = {\n",
    "    'learning_rate': 'learning_rate',\n",
    "    'cross_entropy': 'cross_entropy',\n",
    "    'train_px_accuracy': 'train_px_accuracy',\n",
    "    'train_mean_iou': 'train_mean_iou',\n",
    "    'train_auc' : 'train_auc'\n",
    "  }\n",
    "    x_train_data, y_train_data = importRandomBatch(TOTAL_SIZE,'train', SCALE_RATE)\n",
    "    x_train_data = x_train_data.astype('float16')\n",
    "    y_train_data = y_train_data.astype('uint8')\n",
    "\n",
    "    y_train_data=np.expand_dims(y_train_data,axis=3)\n",
    "\n",
    " \n",
    "    dataset = tf.data.Dataset.from_generator(generator=_generator,\n",
    "                                     output_types=(tf.float32, tf.uint8),\n",
    "                                     output_shapes=(tf.TensorShape(IMG_SHAPE), tf.TensorShape([IMG_SHAPE[0],IMG_SHAPE[1],1])))   \n",
    "\n",
    "    dataset = dataset.shuffle(buffer_size=TOTAL_SIZE)\n",
    "    dataset = dataset.repeat().batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "      tensors=tensors_to_log, every_n_iter=10)\n",
    "    train_hooks = [logging_hook]\n",
    "    eval_hooks = None\n",
    "\n",
    "\n",
    "    tf.logging.info(\"Start training.\")\n",
    "    model.train(\n",
    "      input_fn=lambda: input_fn(True, dataset, BATCH_SIZE, EPOCHS_PER_EVAL),\n",
    "      hooks=train_hooks,\n",
    "      steps=TOTAL_SIZE + 5  # For debug\n",
    "    )\n",
    "\n",
    "    tf.logging.info(\"Epoch ended.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded files input -  100\n",
      "loaded files input -  200\n",
      "loaded files input -  300\n",
      "loaded files input -  400\n",
      "loaded files input -  500\n",
      "loaded files output -  100\n",
      "loaded files output -  200\n",
      "loaded files output -  300\n",
      "loaded files output -  400\n",
      "loaded files output -  500\n",
      "INFO:tensorflow:Restoring parameters from /home/rvygon/data/DeepLab/export/1549978918/variables/variables\n"
     ]
    }
   ],
   "source": [
    "from ImportUtil import *\n",
    "import os, glob, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import predictor\n",
    "import time\n",
    "\n",
    "latest = '/home/rvygon/data/DeepLab/export/1549978918'\n",
    "x_test, y_test_data, files = importBatch(500,0,1,'val', SCALE_RATE)\n",
    "#x_test = ImportImages('/home/rvygon/data/DeepLab/video/leftImg8bit/demoVideo/stuttgart_02')\n",
    "predict_fn = predictor.from_saved_model(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499"
     ]
    }
   ],
   "source": [
    "x_test_data=x_test.astype('float32')\n",
    "pred1=[]\n",
    "z = 0\n",
    "with tf.device('/GPU:2'):    \n",
    "    for i in range(500):\n",
    "        pred = predict_fn({'image':[x_test_data[i]]})\n",
    "        pred1.append(pred)        \n",
    "        upd_print(str(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    upd_print(str(i))\n",
    "    pred2.append(UpscaleImg(np.squeeze(pred1[i]['classes']),2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded files input -  100\n",
      "loaded files input -  200\n",
      "loaded files input -  300\n",
      "loaded files input -  400\n",
      "loaded files input -  500\n",
      "loaded files output -  100\n",
      "loaded files output -  200\n",
      "loaded files output -  300\n",
      "loaded files output -  400\n",
      "loaded files output -  500\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test_data, files = importBatch(500,0,1,'val', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 499 images\n",
      "road : 0.9212177381195837\n",
      "sidewalk : 0.594981511675953\n",
      "building : 0.7892598897746694\n",
      "wall : 0.41246941415226707\n",
      "fence : 0.3664543757741317\n",
      "pole : 0.3375687883346946\n",
      "traffic light : 0.4430339018295128\n",
      "traffic sign : 0.47078896232530343\n",
      "vegetation : 0.8266207041490491\n",
      "terrain : 0.44975340225590327\n",
      "sky : 0.7757676961805746\n",
      "person : 0.41110612135730734\n",
      "rider : 0.5426010468312897\n",
      "car : 0.7872809111354783\n",
      "truck : 0.7057384193241931\n",
      "bus : 0.819708766226983\n",
      "train : 0.9000684512841417\n",
      "motorcycle : 0.6934282493473889\n",
      "bicycle : 0.4851912869871111\n"
     ]
    }
   ],
   "source": [
    "pred2=np.array(pred2)\n",
    "score = eval_preds(pred2,y_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6226910018674917\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 2048, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
